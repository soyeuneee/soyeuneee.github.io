-오늘의 모각포 목표: 파이토치 공부

-참고자료: https://youtube.com/watch?v=C1P7PaleKvU&si=YV7RxV5AePyekaEk

-딥러닝 프레임워크들(텐서플로우(TensorFlow), 케라스(Keras)...) 중 
  파이썬에 가까운 인터페이스를 가진 파이토치.
-파이토치 구성요소:
  torch: 텐서를 생성하는 라이브러리.
  torch.autograd: 자동미분 기능을 제공하는 라이브러리.
  torch.nn: 신경망을 생성하는 라이브러리.
  torch.multiprocessing: 병렬처리 기능을 제공하는 라이브러리.
  torch.utils: 데이터 조작 등 유틸리티 기능을 제공하는 라이브러리.
  torch.legacy(./nn/.optim): Torch로부터 포팅해 온 코드.
  torch.onnx: ONNX(Open Neural Network Exchange)
-텐서(Tensors): 넘파이(NumPy)의 ndarray와 유사.
            GPU를 사용한 연산 가속도 가능.
-"in-place" 방식: in-place 방식으로 텐서의 값을 변경하는 연산 뒤에는 언더바(_) 가 붙는다.

-연산: torch.sub: 뺄셈
       torch.mul: 곱셈
       torch.div: 나눗셈
       torch.mm: 내적(dot product)
-텐서의 조작: 넘파이처럼 인덱싱 가능.
            view: 텐서의 크기(size)나 모양(shape)을 변경.
            item: 텐서에 값이 단 하나라도 존재한다면 숫자값을 얻을 수 있음.
                 스칼라값 하나만 존재해야 함.
            squeeze: 차원을 축소(제거).
            unsqueeze: 차원을 증가(생성).
            stack: 텐서 간 결합.
            cat: 텐서를 결합하는 메소드(concatenate).
                넘파이의 stack와 유사하지만, 쌓을 dim이 존재해야 함.
                 (ex. 해당 차원을 늘려준 뒤 결합.)
            chuck: 텐서를 여러 개로 나눌 때 사용.\
                   몇 개의 텐서로 나눌 것인가.
            split: chuck과 동일한 기능을 갖지만 약간의 차이점 존재.
                 하나의 텐서당 크기값이 얼마인가.
            torch <-> numpy: Torch Tensor(텐서)를 Numpy array(배열)로 변환 가능.
                           (numpy(), from_numpy())
                           (cf. Tensor가 CPU 상에 있다면 Numpy 배열은 메모리 공간을 공유므로 하나가 변화하면, 다른 하나도 변화함.)
-CUDA Tensors: ".to" 메소드를 사용해 텐서를 어떠한 장치(ex. CPU, GPU)로도 이동시킬 수 있음.
-AUTOGRAD(자동미분): autograd 패키지는 Tensor의 모든 연산에 대해 자동 미분을 제공함.
                    이는 코드를 어떻게 작성해 실행하느냐에 따라 역전파가 정의된다는 의미.
                     backprop를 위한 미분값을 자동 계산.
-Tensor: data: tensor 형태의 데이터.
        grad: data가 가져온 layer에 대한 미분값 저장.
        grad_fn: 미분값을 계산한 함수에 대한 정보 저장(어떤 함수에 대해 backprop 했는지)
        requires_grad 속성을 True로 설정하면, 해당 텐서에서 이루어지는 모든 연산에 대해 추적하기 시작.
        계산이 완료된 후, .backward() 를 호출하면 자동으로 gradient를 계산할 수 있으며, .grad 속성에 누적됨.
        기록을 추적하는 것을 중단시키려면, .detach() 를 호출하여 연산기록으로부터 분리시켜야 함.
        기록을 추적하는 것을 방지하기 위해 코드 블럭을 with torch.no_grad(): 로 감싸면 gradient는 필요없지만, requires_grad=True 로 설정되어 학습 가능한 매개변수를 갖는 모델을 평가할 때 유용.
        Aurograd 구현에서 매우 중요하게 쓰이는 Function 클래스.
-기울기(gradient): ".backward()" 를 사용해 역전파 계산 가능.
-nn & nn.functional:
  두 패키지는 동일한 기능을 하지만 방식에서 차이가 있음.
  autograd 관련 작업들을 nn & nn,functional 을 통해 진행할 수 있음.
  텐서를 직접 다룰 때 requires_grad 와 같은 방식으로 진행 가능함.
  결론적으로, torch.nn 은 attribute 를 활용해 state 를 저장하고 활용하며, 
  torch.nn.functional 로 구현한 함수의 경우에는 인스턴스화 시킬 필요 없이 사용이 가능함.
-nn: 주로 가중치(weights), 편향(bias)값들이 내부에서 자동으로 생성되는 레이어들을 사용할 때.
    weight 값들을 직접 선언하지 않음.
    nn 패키지의 예시: Containers
                   Convolution Layers
                   Pooling Layers
                   Padding Layers
                   Non-linear Activations (weighted sum, nonlinearity)
                   Non-linear Activations (other)
                   Normalization Layers
                   Recurrent Layers
                   Transformer Layers
                   Linear Layers
                   Dropout Layers
                   Sparse Layers
                   Distance Functions
                   Loss Functions
                   ...
-nn.function: 가중치를 직접 선언하여 인자로 넣어줘야 함.
            nn.function의 예시: Convolution Functions
                            Pooling Functions
                            Non-linear activation functions
                            Linear functions
                            Dropout functions
                            Sparse functions
                            Distance functions
                            Loss functions 
                            ...
-Torchvision: transforms: 전처리할 떄 사용하는 메소드.
           transforms에서 제공하는 클래스 이외에 일반적으로 클래스를 따로 만들어 전처리 단계를 진행함.
-torch에서는 channel(채널)이 앞에 옴.
 (cf. tensorflow, keras 등에서는 channel이 뒤에 옴(channel last).)
-nn.Conv2d:
  in_channels: channel의 갯수.
  out_channels: 출력 채널의 갯수.
  kernel_size: 커널(필터) 사이즈.
  텐서플로우, 케라스와 다르게 레이어의 input 인자에도 값을 넣어줘야 함.
-F.relu:
  ReLU 함수를 적용하는 레이어.
  nn.ReLU 로도 사용이 가능함.
-Optimizer:
  import torch.optim as optim
  model 의 파라미터를 업데이트.
  .zero_grad() 로 초기화.
  .step() 으로 업데이트.
  
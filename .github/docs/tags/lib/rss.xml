<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[soyeuneee.github.io]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib\media\favicon.png</url><title>soyeuneee.github.io</title><link/></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Wed, 17 Jul 2024 04:09:40 GMT</lastBuildDate><atom:link href="lib\rss.xml" rel="self" type="application/rss+xml"/><pubDate>Wed, 17 Jul 2024 04:09:39 GMT</pubDate><ttl>60</ttl><dc:creator/><item><title><![CDATA[2024.07.16]]></title><description><![CDATA[ 
 <br>-오늘의 모각포 목표: 파이토치 공부<br>-참고자료: <a rel="noopener" class="external-link" href="https://youtube.com/watch?v=C1P7PaleKvU&amp;si=YV7RxV5AePyekaEk" target="_blank">https://youtube.com/watch?v=C1P7PaleKvU&amp;si=YV7RxV5AePyekaEk</a><br>-딥러닝 프레임워크들(텐서플로우(TensorFlow), 케라스(Keras)...) 중<br>
파이썬에 가까운 인터페이스를 가진 파이토치.<br>
-파이토치 구성요소:<br>
torch: 텐서를 생성하는 라이브러리.<br>
torch.autograd: 자동미분 기능을 제공하는 라이브러리.<br>
torch.nn: 신경망을 생성하는 라이브러리.<br>
torch.multiprocessing: 병렬처리 기능을 제공하는 라이브러리.<br>
torch.utils: 데이터 조작 등 유틸리티 기능을 제공하는 라이브러리.<br>
torch.legacy(./nn/.optim): Torch로부터 포팅해 온 코드.<br>
torch.onnx: ONNX(Open Neural Network Exchange)<br>
-텐서(Tensors): 넘파이(NumPy)의 ndarray와 유사.<br>
GPU를 사용한 연산 가속도 가능.<br>
-"in-place" 방식: in-place 방식으로 텐서의 값을 변경하는 연산 뒤에는 언더바(_) 가 붙는다.<br>-연산: torch.sub: 뺄셈<br>
torch.mul: 곱셈<br>
torch.div: 나눗셈<br>
torch.mm: 내적(dot product)<br>
-텐서의 조작: 넘파이처럼 인덱싱 가능.<br>
view: 텐서의 크기(size)나 모양(shape)을 변경.<br>
item: 텐서에 값이 단 하나라도 존재한다면 숫자값을 얻을 수 있음.<br>
스칼라값 하나만 존재해야 함.<br>
squeeze: 차원을 축소(제거).<br>
unsqueeze: 차원을 증가(생성).<br>
stack: 텐서 간 결합.<br>
cat: 텐서를 결합하는 메소드(concatenate).<br>
넘파이의 stack와 유사하지만, 쌓을 dim이 존재해야 함.<br>
(ex. 해당 차원을 늘려준 뒤 결합.)<br>
chuck: 텐서를 여러 개로 나눌 때 사용.<br>
몇 개의 텐서로 나눌 것인가.<br>
split: chuck과 동일한 기능을 갖지만 약간의 차이점 존재.<br>
하나의 텐서당 크기값이 얼마인가.<br>
torch &lt;-&gt; numpy: Torch Tensor(텐서)를 Numpy array(배열)로 변환 가능.<br>
(numpy(), from_numpy())<br>
(cf. Tensor가 CPU 상에 있다면 Numpy 배열은 메모리 공간을 공유므로 하나가 변화하면, 다른 하나도 변화함.)<br>
-CUDA Tensors: ".to" 메소드를 사용해 텐서를 어떠한 장치(ex. CPU, GPU)로도 이동시킬 수 있음.<br>
-AUTOGRAD(자동미분): autograd 패키지는 Tensor의 모든 연산에 대해 자동 미분을 제공함.<br>
이는 코드를 어떻게 작성해 실행하느냐에 따라 역전파가 정의된다는 의미.<br>
backprop를 위한 미분값을 자동 계산.<br>
-Tensor: data: tensor 형태의 데이터.<br>
grad: data가 가져온 layer에 대한 미분값 저장.<br>
grad_fn: 미분값을 계산한 함수에 대한 정보 저장(어떤 함수에 대해 backprop 했는지)<br>
requires_grad 속성을 True로 설정하면, 해당 텐서에서 이루어지는 모든 연산에 대해 추적하기 시작.<br>
계산이 완료된 후, .backward() 를 호출하면 자동으로 gradient를 계산할 수 있으며, .grad 속성에 누적됨.<br>
기록을 추적하는 것을 중단시키려면, .detach() 를 호출하여 연산기록으로부터 분리시켜야 함.<br>
기록을 추적하는 것을 방지하기 위해 코드 블럭을 with torch.no_grad(): 로 감싸면 gradient는 필요없지만, requires_grad=True 로 설정되어 학습 가능한 매개변수를 갖는 모델을 평가할 때 유용.<br>
Aurograd 구현에서 매우 중요하게 쓰이는 Function 클래스.<br>
-기울기(gradient): ".backward()" 를 사용해 역전파 계산 가능.<br>
-nn &amp; nn.functional:<br>
두 패키지는 동일한 기능을 하지만 방식에서 차이가 있음.<br>
autograd 관련 작업들을 nn &amp; nn,functional 을 통해 진행할 수 있음.<br>
텐서를 직접 다룰 때 requires_grad 와 같은 방식으로 진행 가능함.<br>
결론적으로, torch.nn 은 attribute 를 활용해 state 를 저장하고 활용하며,<br>
torch.nn.functional 로 구현한 함수의 경우에는 인스턴스화 시킬 필요 없이 사용이 가능함.<br>
-nn: 주로 가중치(weights), 편향(bias)값들이 내부에서 자동으로 생성되는 레이어들을 사용할 때.<br>
weight 값들을 직접 선언하지 않음.<br>
nn 패키지의 예시: Containers<br>
Convolution Layers<br>
Pooling Layers<br>
Padding Layers<br>
Non-linear Activations (weighted sum, nonlinearity)<br>
Non-linear Activations (other)<br>
Normalization Layers<br>
Recurrent Layers<br>
Transformer Layers<br>
Linear Layers<br>
Dropout Layers<br>
Sparse Layers<br>
Distance Functions<br>
Loss Functions<br>
...<br>
-nn.function: 가중치를 직접 선언하여 인자로 넣어줘야 함.<br>
nn.function의 예시: Convolution Functions<br>
Pooling Functions<br>
Non-linear activation functions<br>
Linear functions<br>
Dropout functions<br>
Sparse functions<br>
Distance functions<br>
Loss functions<br>
...<br>
-Torchvision: transforms: 전처리할 떄 사용하는 메소드.<br>
transforms에서 제공하는 클래스 이외에 일반적으로 클래스를 따로 만들어 전처리 단계를 진행함.<br>
-torch에서는 channel(채널)이 앞에 옴.<br>
(cf. tensorflow, keras 등에서는 channel이 뒤에 옴(channel last).)<br>
-nn.Conv2d:<br>
in_channels: channel의 갯수.<br>
out_channels: 출력 채널의 갯수.<br>
kernel_size: 커널(필터) 사이즈.<br>
텐서플로우, 케라스와 다르게 레이어의 input 인자에도 값을 넣어줘야 함.<br>
-F.relu:<br>
ReLU 함수를 적용하는 레이어.<br>
nn.ReLU 로도 사용이 가능함.<br>
-Optimizer:<br>
import torch.optim as optim<br>
model 의 파라미터를 업데이트.<br>
.zero_grad() 로 초기화.<br>
.step() 으로 업데이트.<br>
]]></description><link>2024.07.16.html</link><guid isPermaLink="false">2024.07.16.md</guid><pubDate>Tue, 16 Jul 2024 11:19:08 GMT</pubDate></item></channel></rss>